# 🧠 한국어 RAG 프로토타입 구축 (온프레미스)
### 모델 조합: `jhgan/ko-sbert-sts` + `google/flan-t5-base`
### 구성: Docker Compose + pgvector + 임베딩 서비스 + LLM 서비스 + RAG 코드


# 개발 진행 중.... 완료 후 수정 예정

좋아. “행정망/온프레미스 + pgvector + (지금은 Ollama) + 나중에 확장”을 전제로, **코드 없이 설계 뼈대**를 말로 잡아줄게.

---

## 1) 전체 구성(작게 시작, 나중에 키우기 쉬운 형태)

### 컴포넌트 4개

1. **RAG API 서버** (너희 서비스의 본체)

    * 문서 업로드/적재
    * 질의/응답
    * 권한/감사로그
2. **LLM/임베딩 서빙 서버** (초기: Ollama, 나중: vLLM로 교체 가능)

    * `/chat`(답변 생성), `/embeddings`(벡터 생성) 같은 내부 API 제공
3. **PostgreSQL + pgvector**

    * chunk 텍스트 + 임베딩 + 메타데이터 + 권한정보 저장
4. **파일 저장소**

    * 원문(PDF/HWP/DOCX) 보관 (NAS/MinIO/파일서버 아무거나)
    * pg에는 원문을 넣기보다 “경로/해시/버전”만 저장

> 핵심 포인트: “모델 서빙”을 RAG API와 분리해두면, Ollama → vLLM 교체가 쉬워짐.

---

## 2) 폴더/모듈 구조(서비스 내부 설계)

RAG API 서버를 논리적으로 5개 모듈로 나눠.

### A. ingestion 모듈 (적재 파이프라인)

* 문서 등록/업로드
* 텍스트 추출
* 청킹
* 임베딩 생성 호출
* DB 저장

**입력:** 원문 파일 + 문서 메타(제목/부서/등급/작성일 등)
**출력:** chunk 레코드 + 벡터 + 인덱싱 상태

### B. retrieval 모듈 (검색 파이프라인)

* 질의 임베딩 생성
* pgvector Top-k 검색
* 권한 필터(중요)
* (선택) rerank(재정렬)
* 컨텍스트 패키징(LLM 프롬프트에 넣을 “근거 묶음” 만들기)

### C. generation 모듈 (답변 생성)

* “시스템 프롬프트 + 유저 질문 + 컨텍스트”를 합성
* LLM 호출
* 결과 후처리(출처 표기, 금칙어/개인정보 마스킹, 길이 제한 등)

### D. governance 모듈 (운영/통제)

* 사용자/조직/권한(RBAC/ABAC)
* 문서 등급 정책(예: 대외비/내부/공개)
* 감사 로그(누가 어떤 문서 근거로 답을 봤는지)

### E. eval/ops 모듈 (품질/운영)

* 질의-응답 기록(옵션)
* 검색 실패율/무응답률
* 응답시간/토큰 사용량(온프레미스여도 측정은 필요)
* “좋아요/싫어요 + 이유” 같은 피드백 수집

---

## 3) DB 스키마(핵심 테이블 설계만, 말로)

### 1) documents (원문 단위)

* doc_id
* title
* source_type(업로드/크롤/내부시스템 연계 등)
* file_path(파일서버 경로) + file_hash(무결성)
* security_level(등급)
* owner_org / created_by / created_at
* status(UPLOADED → PARSED → EMBEDDED → READY / FAILED)

### 2) chunks (검색 단위)

* chunk_id, doc_id(FK)
* chunk_text
* embedding(vector)
* chunk_index(문서 내 순서)
* page_range / section (가능하면)
* tokens_count(대략)
* metadata(json) (유연하게: 장/절, 키워드 등)

### 3) doc_acl 또는 policy (권한)

* doc_id
* allowed_org / allowed_role / allowed_user
* deny 규칙이 필요하면 별도 테이블

> 권한은 “검색 결과에서” 반드시 필터링해야 함.
> (LLM이 컨텍스트를 받기 전에 차단)

### 4) queries / answers / audit_log (운영 로그)

* 누가, 언제, 어떤 질문을 했고
* 어떤 chunk/doc가 근거로 사용됐는지
* 결과가 뭐였는지(옵션: 원문 저장 금지 정책이면 해시만)

---

## 4) “LangChain을 어디에 두는가?” (확장형 핵심)

LangChain은 **retrieval + generation 모듈을 조립하는 자리**에 둬.

* **Query Router**: 질문이 “규정/매뉴얼/회의록/코드/통계” 중 어디에 가까운지 분기
* **Retriever Chain**: pgvector 검색 → 권한 필터 → rerank → 컨텍스트 생성
* **Answer Chain**: 답변 생성 + 출처 표기 규칙 강제
* **Tooling 확장 포인트**:
  나중에 “내부 DB 조회”, “업무 시스템 API”, “표/수치 계산” 같은 걸 도구로 붙일 수 있음

즉 LangChain을 쓰는 이유는 “지금 RAG만”이 아니라 **나중에 업무 흐름(툴/에이전트/분기)을 얹기 위해서**야.

---

## 5) LlamaIndex는 어디에 두면 좋은가?

선택이지만, 가장 좋은 자리는 보통 **ingestion(문서 적재)** 쪽이야.

* 문서 로더/파서/청킹 파이프라인이 깔끔함
* “문서 구조(페이지/섹션) 메타”를 잡아두기 좋음

권장 패턴:

* **LangChain이 서비스의 “질의/응답 orchestration”**
* **LlamaIndex가 “문서 적재/청킹/인덱싱 유틸”**

단, LlamaIndex 없이도 ingestion은 만들 수 있어.
(다만 팀이 빨리 안정화하려면 LlamaIndex가 편한 경우가 많음)

---

## 6) 모델 서빙(Ollama → vLLM) 교체를 쉽게 하는 방법

### “LLMClient”라는 내부 인터페이스를 하나로 고정

* `embed(text) -> vector`
* `chat(messages, options) -> answer`

그리고 실제 구현체만 바꿔:

* 초기: **OllamaClient**
* 확장: **VLLMClient**
* 더 나중: 다른 서빙엔진도 같은 인터페이스로 교체 가능

이렇게 하면 LangChain 체인과 DB 로직은 그대로 두고,
**모델 엔진만 갈아끼우는 구조**가 됨.

---

## 7) 운영 관점에서 꼭 넣어야 하는 “안전장치”

행정망이면 특히 중요해.

* **권한 필터는 검색 결과 단계에서 강제**
* **출처(문서/페이지) 표기 의무화**: “근거 없으면 답변하지 말기” 옵션
* **PII/민감정보 마스킹 정책**(필요 시)
* **문서 버전 관리**: 같은 문서가 업데이트되면 doc_id/version 분리
* **재색인/재임베딩 작업 큐**: 문서 대량 적재 시 비동기로(작아도 큐 개념은 있으면 편함)

---

## 8) 최종 추천: 너 상황(작고, 확장 고려)에 맞는 결론

* **프로토타입부터 LangChain으로 “질의 파이프라인 뼈대”를 잡는 건 좋은 선택**
* ingestion은

    * 빠르게 가려면 **LlamaIndex를 유틸로 써서 안정화**
    * 단순하면 직접 구현해도 됨
* 모델은

    * 초기: Ollama로 시작
    * 동시성/성능 필요해지면 vLLM로 교체(위 인터페이스 유지)

---

원하면 내가 다음 메시지에서(역시 코드 없이) **“요청/응답 API 설계”**까지 말로 잡아줄게:

* `/documents/upload`, `/documents/{id}/reindex`
* `/chat/query` (top-k, rerank, security_level 옵션)
* `/admin/metrics`, `/admin/audit`
  이런 식으로 실제 구축 단계에서 바로 작업 분담 가능한 수준으로.




---

# 📌 1. 전체 구성 개요

이 문서는 다음 모델 기반의 **온프레미스 RAG 프로토타입** 구축 과정을 설명합니다.

- **KoSentenceBERT (jhgan/ko-sbert-sts)** → 임베딩 생성 (768차원)
- **FLAN-T5-base (google/flan-t5-base)** → 응답 생성(요약/정리)
- **pgvector** → 임베딩 벡터 저장·검색
- **FastAPI** → 임베딩 및 LLM 서비스 API 구성
- **docker-compose** → 두 개의 모델 서비스 + DB 실행
- **ask.py** → RAG: 질문 → 임베딩 → 검색 → 응답 생성


---

# 📌 2. 전체 폴더 구조

project-root/
├── docker-compose.yml
├── embedding-service/
│ ├── Dockerfile
│ └── app.py
├── llm-service/
│ ├── Dockerfile
│ └── app.py
├── ingest_pdf.py # PDF를 pgvector에 저장
└── ask.py # 질문 → 검색 → 응답

Docker 전체 실행
docker-compose up -d --build

PDF 인덱싱
python ingest_pdf.py

3) 질문하기
   python ask.py