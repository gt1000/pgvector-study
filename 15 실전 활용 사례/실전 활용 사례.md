# 온프레미스 RAG 구현 기준 AI 아키텍처 설계서 (최종)
## 해경 수색구조 · 해양수산/양식 도메인 / Python 중심

---

## 0. 이 문서의 목적
온프레미스(폐쇄망) 환경에서 RAG를 “실제로 구현/운영”하기 위한 **AI 기술 아키텍처**를 정리한다.  
특히 다음 항목을 명확히 한다.

- LangChain / LangGraph / LlamaIndex / LM Studio / Ollama / vLLM 각각의 역할
- “최적 조합(권장)”과 “왜 그 조합인지” (선택 이유)
- Python 서비스/모듈 구조(폴더 구조)와 각 역할
- 벤치마킹(품질/성능/운영성)으로 선택을 검증하는 방법

---

## 1. 용어 정리 (이 부분만 이해하면 전체가 잡힘)

### 1.1 모델 서빙(Serving) 계층: “모델을 돌려주는 엔진”
- **Ollama**: 로컬/온프레미스에서 모델 실행을 쉽게 해주는 런타임(설치/운영 쉬움)
- **vLLM**: 운영형 고성능 LLM 서빙(동시성/배치 처리/처리량에 강함)
- **LM Studio**: 개발/테스트 친화적인 데스크톱 런타임(빠른 실험에 유리)

> 결론:  
> - “운영 서버”는 **vLLM(가능하면)** 또는 **Ollama(간편/소규모)**  
> - **LM Studio는 개발/PoC용**으로 매우 좋지만, 운영 표준으로 고정하기엔 조직/서버 운영 관점에서 애매한 경우가 많다.

---

### 1.2 RAG 프레임워크 계층: “문서 인입 → 검색 → 프롬프트 구성 → 응답 생성”
- **LlamaIndex**: “데이터/인덱싱/RAG”에 특화된 프레임워크  
  - 문서 로더, 청킹, 인덱싱, 리트리버, 응답 합성(Answer Synthesis)이 촘촘함
- **LangChain**: “LLM 앱” 구성 요소가 풍부한 프레임워크  
  - 다양한 툴/커넥터/체인 구성에 강점(레고 블럭 느낌)
- **LangGraph**: “흐름(워크플로우/상태머신)” 제어에 특화  
  - 멀티스텝 RAG(분류→검색→재검색→요약→검증→출처정리)를 안정적으로 구현

> 결론:
> - LlamaIndex/LangChain은 “기능 라이브러리”
> - LangGraph는 “오케스트레이션(흐름 제어)”  
> - 실무에서 품질이 올라가는 지점은 대부분 “흐름 제어(조건 분기/재시도/검증)”에서 나온다.

---

## 2. 권장 “최적 조합” (온프레미스/소규모 사용자 기준)

### 2.1 추천 조합 (권장안)
- **(서빙)** Ollama 또는 vLLM  
- **(RAG 데이터/검색)** LlamaIndex  
- **(워크플로우/오케스트레이션)** LangGraph  
- **(API)** FastAPI (또는 Spring이 이미 강하면 Spring+Python 분리 가능)

즉,
- “문서 인입/검색은 LlamaIndex로 단단하게”
- “복잡한 단계 제어는 LangGraph로 깔끔하게”
- “모델은 OpenAI-compatible처럼 호출(서빙 엔진 교체 가능하게)”
- “API는 FastAPI로 단일화하면 가장 단순”

---

### 2.2 왜 LangChain을 메인으로 안 잡고 LlamaIndex 중심이냐?
온프레미스 RAG에서 가장 많은 시간이 들어가는 건 “문서 인입/청킹/인덱싱/리트리벌 품질”이다.
- LlamaIndex는 이 부분(ingestion + retrieval)이 구조적으로 더 촘촘하고 빠르게 안정화됨
- LangChain은 범용성이 강해서 좋지만, “데이터 인덱싱 파이프라인”을 처음부터 조립할 때 실수 여지가 커짐

> 단, LangChain은 “툴/커넥터/에이전트”가 필요해지는 2단계에서 강력해진다.  
> 그래서 “초기 운영형 RAG”는 LlamaIndex 중심이 비용 대비 효율이 좋다.

---

### 2.3 Ollama vs vLLM 선택 기준
- **Ollama 추천 상황**
  - 사용자 수 적음, 문서 규모 작음, “쉽게 띄우고 운영”이 목표
  - GPU가 없거나 제한적
- **vLLM 추천 상황**
  - 동시 요청이 증가할 가능성이 큼
  - GPU 서버가 있고 “지연시간/처리량”이 중요
  - 운영 표준(서버/서비스)로 고정하려는 목표

---

### 2.4 LM Studio는 어디에 쓰는가?
- **PoC/개발 환경에서만**
  - “모델 바꿔가며 응답 품질 확인”
  - “프롬프트 튜닝”
  - “임베딩/LLM API를 임시로 빠르게 제공”
- 운영에서는 보통 **Ollama 또는 vLLM로 동일 API 형태를 유지**하는 것이 관리가 쉽다.

---

## 3. AI 시스템 아키텍처 (구현 관점)

### 3.1 서비스 분해(권장)
온프레미스 소규모 환경이면 2개로 끝내도 된다.

1) **rag-api** (온라인)
- 질의응답 API
- 검색 수행
- 프롬프트 구성
- 모델 호출
- 출처 정리 및 반환
- 로그/감사 저장

2) **ingest-worker** (오프라인/배치)
- 문서 업로드/수집
- 텍스트 추출/정제
- 청킹/메타 추출(페이지/조항/섹션)
- 임베딩 생성
- 벡터DB(pgvector)에 적재
- 재인덱싱/버전 관리

> 이렇게 나누면 “사용자 응답(온라인)”과 “문서 처리(배치)”가 서로 리소스를 잡아먹지 않는다.

---

### 3.2 RAG 처리 파이프라인(표준)
**Ingestion**
- 문서 로드 → 텍스트 추출 → 정제 → 청킹 → 임베딩 → 저장

**Query**
- 질의 정규화 → 검색(벡터/키워드/하이브리드) → 재정렬(옵션) → 컨텍스트 구성 → 답변 생성 → 근거 구성

---

### 3.3 품질을 올리는 “워크플로우(=LangGraph가 빛나는 지점)”
단순 RAG(검색→답변)만으로는 현장에서 다음 문제가 생김
- 질문이 너무 짧음/애매함
- 출처가 부정확하거나 부족함
- 도메인별 답변 톤/형식이 섞임
- 문서에 없는 내용을 모델이 만들어냄(환각)

그래서 “운영형”에서는 보통 아래 단계가 들어간다:

1) **Query Router**: 질문 분류(SAR/양식/법령/장비/일반)
2) **Retriever**: 분류 결과에 맞는 인덱스/필터로 검색
3) **Evidence Check**: 근거가 충분한지 검사(부족하면 재검색/확장검색)
4) **Answer Synthesis**: 답변 생성
5) **Citation Formatter**: 출처 표준 포맷으로 정리(문서명/페이지/조항)
6) **Refusal Policy**: 근거가 없으면 “확인 불가” 규칙 적용

LangGraph는 이걸 “조건 분기 + 재시도”로 깔끔하게 만들기 좋다.

---

## 4. Python 폴더/모듈 구조(권장안)

### 4.1 레포 구조(2서비스 기준)
```

rag-platform/
rag_api/
app/
main.py
api/                      # FastAPI routers
chat.py
docs.py
admin.py
core/                     # 설정/공통
config.py
logging.py
exceptions.py
rag/                      # 질의응답 핵심
graph/                  # LangGraph 워크플로우
workflow.py
nodes/
route_query.py      # 질문 분류/라우팅
retrieve.py         # 검색 실행
evidence_check.py   # 근거 충분성 판단/재검색
synthesize.py       # 답변 생성
format_citations.py # 출처 정리
prompts/                # 도메인별 프롬프트 템플릿
sar.yaml
aquaculture.yaml
law.yaml
retrieval/              # 검색 모듈(LlamaIndex/커스텀)
retriever.py
reranker.py           # 옵션(없어도 됨)
query_expansion.py    # 옵션(동의어/약어 룰)
llm/                    # 모델 호출 클라이언트(OpenAI 호환)
client.py             # chat/completions
embeddings.py         # embeddings
models.py             # 요청/응답 DTO
policy/                 # 안전장치(운영 규칙)
refusal.py            # 근거 없으면 거절
pii_masking.py        # 민감정보 마스킹(선택)
store/                    # 저장소 어댑터
pgvector_store.py
file_store.py
auth/                     # 인증/권한(선택)
rbac.py
observability/            # metrics/trace(선택)
metrics.py

ingest_worker/
app/
main.py
pipeline/
ingest_job.py
steps/
load_document.py      # 파일/폴더/업로드 문서 로드
extract_text.py       # PDF/HWP/DOCX 추출기
clean_text.py         # 헤더/푸터 제거, 정규화
chunk_text.py         # 청킹 정책
embed_chunks.py       # 임베딩 생성
upsert_store.py       # pgvector 적재
build_metadata.py     # 페이지/조항/섹션 메타 생성(가능하면)
parsers/                  # 포맷별 파서
pdf.py
docx.py
hwp.py                  # 환경에 맞게 구현(변환 기반도 가능)
chunking/
policy.py
dedup/
hash.py                 # source_hash 기반 중복/변경 감지
config.py

shared/
schemas/                    # 공통 DTO/스키마(선택)
utils/
README.md

```

---

## 5. 각 모듈의 “역할” (현장에서 필요한 관점)

### 5.1 rag_api (온라인)
- API 서버(질의/응답, 출처 반환)
- 검색/재검색/근거 점검/포맷팅
- 정책 적용(근거 없으면 거절, 민감정보 마스킹)
- 로그/감사(누가 무엇을 물었고 어떤 문서를 근거로 답했는지)

### 5.2 ingest_worker (오프라인)
- 문서가 들어오면 “RAG가 검색 가능한 형태”로 만들어 DB에 적재
- 배치로 돌리기 때문에 GPU/CPU를 더 강하게 써도 됨
- 장애/재시도/부분 재인덱싱이 중요

---

## 6. “수많은 모델”을 어떻게 다뤄야 하는가? (운영형 모델 전략)

### 6.1 최소 모델 구성(권장)
운영은 단순해야 한다. 처음부터 모델을 많이 깔면 망한다.

- **임베딩 모델 1개** (고정)
- **LLM 모델 1개** (고정)
- (선택) **재랭커 1개** (필요해질 때만)

> 핵심: 임베딩 모델을 바꾸면 “전체 재인덱싱”이 필요하다.  
> 그래서 임베딩 모델은 특히 더 신중하게 한 번에 고른다.

---

## 7. 벤치마킹 기반 “선택 이유”를 만드는 방법 (이게 최종 답)

### 7.1 벤치마킹이 필요한 이유
“어떤 조합이 최적이냐”는 말로 끝나지 않는다.  
해경/도메인 문서에서는 실제 질문에 대해 **근거가 맞는지**가 승부다.

따라서 다음 3가지를 동시에 평가해야 한다:
1) 검색 품질 (근거를 제대로 가져오나)
2) 생성 품질 (근거 기반으로 제대로 요약/답변하나)
3) 운영 성능 (지연시간/처리량/자원)

---

### 7.2 최소 평가셋(권장)
- 질문 50~200개만 만들어도 초기 PoC에는 충분
- 도메인별로 균형:
  - SAR 절차 질문
  - 장비 운용 질문
  - 법령/지침 근거 질문
  - 양식/수산 운영 질문
- 각 질문마다 “정답 문서(또는 정답 청크)”를 지정(최소 1개)

---

### 7.3 평가 지표(운영형으로 필요한 것만)
**Retrieval**
- Recall@K: 정답 근거가 Top-K 안에 들어오나?
- MRR: 정답이 얼마나 위에 있나?

**Answer**
- Faithfulness(근거 충실성): 근거에서 벗어난 말이 있나?
- Citation Coverage: 답변에 출처가 제대로 붙었나?

**Performance**
- p95 latency: 95% 요청의 응답시간
- Throughput: 초당 처리량
- 자원: CPU/GPU 메모리 사용량

---

### 7.4 실험 설계(조합을 “증명”하는 방식)
아래는 실제로 선택 이유를 만들기 위한 최소 실험(AB 테스트)이다.

1) 임베딩 모델 2개 후보 비교
- 같은 청킹 정책, 같은 검색 K
- Recall@10, MRR 비교

2) 청킹 정책 2개 비교
- 800~1500자 vs 조항/단계 중심 구조 청킹
- SAR/법령 문서에서 체감 차이가 큼

3) RAG 워크플로우 비교
- 단순 RAG(검색→답변) vs LangGraph(근거 부족 시 재검색)
- 환각 감소/출처 품질에서 차이가 남

4) 서빙 엔진 비교(운영 성능)
- Ollama vs vLLM (p95 latency, 동시성)
- 사용자 수가 늘 가능성이 있으면 vLLM이 선택 근거가 됨

---

## 8. 최종 권장 “실무형 결론”
소규모/온프레미스/공공 환경에서 실패 확률이 가장 낮은 조합은 아래다.

- **서빙**: Ollama(간편) → 성장하면 vLLM로 교체
- **RAG 데이터/검색**: LlamaIndex(인입/인덱싱/리트리벌 안정)
- **워크플로우**: LangGraph(근거 부족 시 재검색/검증/정책 적용)
- **API**: FastAPI 단일(가장 단순), 또는 조직 표준이 Spring이면 Spring+Python 분리도 가능

> 핵심은 “특정 기술이 좋다”가 아니라  
> **(1) 운영 단순성 (2) 품질 검증 가능성 (3) 교체 가능성**이다.  
> 이 3가지를 만족하는 조합이 최적이다.

---

## 9. 다음 단계(바로 구현으로 연결)
원하면 이 문서를 기반으로 다음 산출물을 바로 뽑아낼 수 있다.

- 실제 `rag_api` FastAPI 스켈레톤 코드(라우터/그래프 노드/클라이언트)
- ingest_worker 배치 파이프라인 스켈레톤
- 모델 서버(Ollama/vLLM) 연동 규격(OpenAI 호환 요청/응답 DTO)
- 벤치마킹 스크립트(eval) 구조(질문셋/지표/리포트 출력)

---
```
