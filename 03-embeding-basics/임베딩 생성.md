# 📘 03. 임베딩 생성 (Embedding Basics)

임베딩(Embedding)은 텍스트, 이미지, 오디오와 같은 비정형 데이터를 **숫자 벡터 형태로 변환**하여  
AI 모델이 의미 기반 검색, 추천, 분류 등 다양한 작업을 수행할 수 있게 하는 핵심 기술입니다.

> 🔎 **중요:**  
> pgvector는 **임베딩을 생성하지 않습니다.**  
> 임베딩 생성은 반드시 **DB 바깥(애플리케이션 · 임베딩 서비스 · LLM 서버)** 에서 수행해야 합니다.  
> pgvector의 역할은 **이미 생성된 임베딩을 저장하고, 인덱싱하며, 유사도 검색을 수행하는 것**입니다.
>
> 또한 **데이터를 저장할 때 사용한 임베딩 모델과, 사용자 질의를 임베딩할 때 사용하는 모델은 반드시 동일해야 합니다.**  
> 모델이 다르면 차원 수가 다르거나 의미 공간이 달라져 **유사도 검색이 무의미해지거나 잘못된 결과를 반환합니다.**

---

## 1) 전체 처리 흐름

pgvector를 활용한 전형적인 RAG / 의미 기반 검색 파이프라인은 다음과 같습니다.

**원본 데이터 → 임베딩 생성 → pgvector에 저장 → 쿼리 텍스트 임베딩 생성 → pgvector 유사도 검색 → 결과 + LLM 응답**

① 원본 문서/텍스트/이미지 수집  
② 외부 임베딩 모델로 벡터 생성 (임베딩 서비스)  
③ 생성된 벡터를 PostgreSQL + pgvector에 저장  
④ 사용자가 질의(자연어 쿼리)를 입력하면 동일한 모델로 임베딩 생성  
⑤ pgvector에서 `ORDER BY embedding <-> :query_vec` 로 유사도 검색  
⑥ 검색 결과를 그대로 보여주거나, LLM에게 전달해 **요약·답변·추천** 생성

---

## 2) 왜 텍스트 임베딩이 필요한가?

### AI는 텍스트의 의미를 직접 이해하지 못합니다

컴퓨터와 AI 모델은 문자를 "의미"로 이해하지 못하고, **단순 문자열 매칭**만 가능합니다.

#### ❌ 단순 텍스트 비교의 문제점

**문제 1: 단어가 다르면 못 찾음**

```
질문: "최적 온도는?"

청크 1: "최적 온도는 25도입니다"  ← "최적 온도" 단어 일치 ✅
청크 2: "실험 결과 섭씨 25도에서 가장 좋은 성과"  ← 단어 불일치 ❌

→ 청크 2가 더 상세한 답변인데 못 찾음!
```

**문제 2: 유사어/동의어 처리 불가**

```
질문: "가격이 얼마인가요?"

청크 1: "가격은 10만원입니다"  ← "가격" 일치 ✅
청크 2: "비용은 10만원입니다"  ← "비용" = 가격 (의미 같음) 하지만 ❌
청크 3: "10만원에 판매합니다"   ← 의미는 같지만 단어 없음 ❌
```

**문제 3: 문맥 이해 불가**

```
질문: "사과가 맛있나요?"

청크 1: "사과는 달고 맛있습니다"  ← 과일 사과 ✅
청크 2: "실수를 사과했습니다"     ← 사과(apology) ❌

→ 단어는 같지만 의미가 다름
```

---

### ✅ 임베딩의 해결책: 의미를 숫자로 변환

**임베딩은 "의미"를 숫자 벡터로 표현합니다.**

```python
# 같은 의미 → 비슷한 숫자
"최적 온도는 25도" 
→ [0.82, 0.09, 0.88, 0.15, ...]

"실험 결과 섭씨 25도에서 가장 좋은 성과"
→ [0.80, 0.11, 0.86, 0.14, ...]
   ↑ 의미가 비슷하면 숫자도 비슷!

# 거리 계산
cosine_distance = 0.02  ← 매우 가까움! (유사함)
```

**핵심:**
- 단어가 달라도 **의미가 같으면** 숫자가 비슷
- AI 모델이 "의미"를 학습해서 벡터로 변환
- 벡터 간 거리 = 의미 유사도

---

### 속도 비교

#### AI가 텍스트를 직접 비교하면? (❌ 불가능)

```
질문: "최적 온도는?"
40개 청크 검색:

1. AI야, 질문과 청크1 유사한가? (LLM 호출 1초)
2. AI야, 질문과 청크2 유사한가? (LLM 호출 1초)
...
40. AI야, 질문과 청크40 유사한가? (LLM 호출 1초)

총 시간: 40초
비용: API 호출 40번 = $$$
```

#### 임베딩 + 벡터 검색 (✅ 현실적)

```
질문: "최적 온도는?" 
→ 임베딩 변환 (0.1초)

pgvector에서 벡터 비교:
- 40개 벡터와 거리 계산 (0.01초)
- 상위 3개 선택

총 시간: 0.11초 (360배 빠름!)
비용: API 호출 1번만
```

---

### 실제 검색 과정

```
1. 데이터 준비 (1회만):
   논문 8페이지 → 40개 청크로 분할
   각 청크 → 임베딩 생성 (의미를 숫자로)
   DB 저장: chunk_text(원문) + embedding(의미)

2. 검색 (매번):
   질문 "최적 온도는?" 
   → 임베딩 생성 [0.8, 0.1, 0.9, ...] (0.1초)
   
   pgvector 벡터 검색:
   - 40개 벡터와 거리 계산 (0.01초)
   - 가장 가까운 3개 선택
   
   결과:
   - 청크5: 거리 0.02 (98% 유사)
   - 청크12: 거리 0.05 (95% 유사)
   - 청크23: 거리 0.08 (92% 유사)

3. 원문 출력:
   찾은 청크의 chunk_text를 사용자에게 보여줌
   또는 LLM에 전달해서 답변 생성
```

---

### 정리

| 방법 | 시간 | 비용 | 의미 이해 | 실용성 |
|------|------|------|----------|--------|
| **텍스트 직접 비교** | 40초 | $$$ | ✅ | ❌ 불가능 |
| **임베딩 벡터 검색** | 0.1초 | $ | ✅ | ✅ 현실적 |

**임베딩이 필요한 이유:**
1. **의미 이해:** 단어가 달라도 의미가 같으면 찾음
2. **속도:** 360배 빠름 (숫자 계산만)
3. **비용:** API 호출 1회로 충분
4. **정확도:** 유사어, 문맥, 의역 모두 처리

---

## 3) 텍스트 임베딩 생성

텍스트 임베딩(Text Embedding)은 문장, 단어, 문서를 숫자 벡터로 변환하는 과정입니다.

### 🔹 예: OpenAI API로 임베딩 생성

```python
from openai import OpenAI

client = OpenAI()

response = client.embeddings.create(
    model="text-embedding-3-small",
    input="안녕하세요."
)

embedding = response.data[0].embedding
print(len(embedding))   # 1536차원
```

### 예: SentenceTransformers (온프레)

```python
from sentence_transformers import SentenceTransformer
import psycopg2

# 1) 최신 임베딩 모델 로드 (추천)
model = SentenceTransformer("nomic-ai/nomic-embed-text-v1.5")

# 2) 텍스트 준비
text = "Feeding rate significantly impacts fish growth under varying temperature."

# 3) 임베딩 생성 (numpy → list)
vec = model.encode(text)
embedding_list = vec.tolist()

# 4) PostgreSQL 연결 및 INSERT
conn = psycopg2.connect(... 생략 ...)
cur = conn.cursor()

# 5) INSERT 실행 (차원 수정 필요: 768)
sql = """
    INSERT INTO research_docs (chunk_text, embedding)
    VALUES (%s, %s::vector(768))
"""
cur.execute(sql, (text, embedding_list))
```

이렇게 생성한 `embedding` 벡터를 그대로 `vector(n)` 컬럼에 저장하면 된다.

---

## 4) 이미지 임베딩 생성

이미지 임베딩은 이미지의 시각적 특징을 벡터로 표현한 것입니다.  
검색, 유사 이미지 탐지, RAG(이미지 기반) 시스템 등에서 활용됩니다.

### 🔹 예: CLIP 모델로 이미지 임베딩 생성

```python
import torch
import clip
from PIL import Image

model, preprocess = clip.load("ViT-B/32")

image = preprocess(Image.open("sample.jpg")).unsqueeze(0)
with torch.no_grad():
    embedding = model.encode_image(image)

embedding = embedding[0].tolist()
print(len(embedding))  # 512차원
```

---

## 5) float32 / half / binary 임베딩 비교

pgvector는 여러 타입의 벡터를 지원합니다.

| 타입 | 설명 | 장점 | 단점 |
|------|------|------|------|
| **vector (float32)** | 32bit float 벡터 | 가장 정확함 | 저장 공간/메모리 사용량 큼 |
| **halfvec (float16)** | 16bit float | 용량 약 50% 절감 | 약간의 정확도 손실 |
| **bit (binary)** | 이진 양자화 벡터 | 매우 빠르고 작음 | 정밀도 손실 크고, 튜닝 필요 |

### 왜 다양한 타입을 사용하는가?

- **float32** → 검색 정확도 최우선, 데이터 규모가 상대적으로 작거나 GPU/메모리 여유가 있을 때
- **half** → 정확도는 크게 떨어지지 않으면서 메모리 절약이 필요한 중대형 서비스
> **정확도 손실이 작은 이유**   
벡터 검색에서는 절대적인 값보다 **상대적인 거리/순위**가 중요하므로 Top-10 결과 중 8~9개는 float32와 동일합니다.
> ```
  > float32: 1.234567890... (소수점 7자리)
  > float16: 1.234500000... (소수점 3-4자리)
> ```
- **binary** → 수억~수십억 벡터를 다루는 초대규모 시스템, 빠른 1차 거친 검색(coarse search)에 유리. 대략적인 방향만 잡음

---

## 6) 임베딩 차원(Dimension)

임베딩은 모델마다 고정된 차원을 갖습니다. (예: 384, 768, 1024, 1536, 3072 등)

| 모델 | 차원 | 설명 |
|------|------|------|
| MiniLM | 384 | 가볍고 빠른 범용 임베딩 |
| KoSentenceBERT | 768 | 한국어 특화 문장 임베딩 |
| BERT-base | 768 | 전통 NLP 모델, 실험/연구용 |
| OpenAI text-embedding-3-small | 1536 | 범용 검색/RAG에 최적 |
| OpenAI text-embedding-3-large | 3072 | 고정확도, 데이터 품질이 중요한 서비스용 |

### 차원이 크면?

- 더 많은 의미/문맥을 담을 수 있어 표현력이 좋아짐
- 하지만 메모리, 저장 공간, 연산 비용도 함께 증가
- 서비스 특성과 예산에 맞춰 **차원 수와 모델을 선택**해야 한다.

---

## 7) 클라우드 / 온프레미스 환경별 임베딩 모델 추천

아래 표는 **pgvector를 사용할 때 많이 쓰이거나, 실무에서 권장되는 임베딩 모델**을 환경별로 정리한 것입니다.

| 환경 | 모델명 | 차원 수 | 특징 / 용도 |
|------|--------|--------|-------------|
| **클라우드** | OpenAI `text-embedding-3-small` | 512~1536 | 범용 RAG/검색에 가장 많이 사용, 비용 대비 성능 우수 |
| **클라우드** | OpenAI `text-embedding-3-large` | 256~3072 | 최고 성능, 차원 조절 가능 (MRL 지원) |
| **클라우드** | Cohere `embed-v3` | 1024 | 다국어 지원, 압축 기능, 하이브리드 검색 최적화 |
| **온프레미스** | `nomic-ai/nomic-embed-text-v1.5` | 768 | 2024년 SOTA급 오픈소스, MRL 지원, 가볍고 빠름 |
| **온프레미스** | `BAAI/bge-m3` | 1024 | 다국어, 하이브리드 검색 지원, 높은 정확도 |
| **온프레미스** | `Alibaba-NLP/gte-Qwen2-7B` | 3584 | MTEB 벤치마크 최상위, 최고 성능 (GPU 필요) |

> 📌 실제 프로젝트에서는  
> "서비스 특성(한국어/다국어), 예산, 레이턴시, GPU 보유 여부"  
> 를 기준으로 위 모델 중 1~2개를 선택한 뒤,  
> 해당 모델의 차원 수에 맞게 **`vector(n)` 컬럼을 정의**하면 된다.

---

## 8) 외부 임베딩 모델 활용과 pgvector 연계

pgvector는 임베딩 생성 기능이 없고, **외부 모델로 생성한 임베딩을 저장/검색하는 기능만 제공**합니다.

### 대표적인 임베딩 생성 모델과 연계 방식

#### 📌 SentenceTransformers (Python, 오픈소스)
- 빠르고 무료, 다양한 언어/도메인 지원
- 온프레미스 환경에서 가장 많이 사용
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("BAAI/bge-m3")  # 2024년 최신 다국어
vec = model.encode("안녕하세요.")
```

#### 📌 OpenAI Embedding API (클라우드)
- 정확도 우수, RAG/검색용으로 설계
- 클라우드 기반 서비스에 적합
```python
from openai import OpenAI

client = OpenAI(api_key="your-api-key")
response = client.embeddings.create(
    model="text-embedding-3-small",
    input="example"
)
vec = response.data[0].embedding
```

#### 📌 CLIP (이미지/텍스트 멀티모달)
- 이미지 검색 / 비디오 분석 / 멀티모달 RAG에서 활용
- 임베딩은 pgvector에 저장, 원본 파일은 객체 스토리지나 NAS에 저장
```python
from transformers import CLIPProcessor, CLIPModel

model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")

inputs = processor(text=["a photo of a cat"], return_tensors="pt")
vec = model.get_text_features(**inputs)
```

---

# 🎯 요약

- pgvector는 **임베딩을 생성하지 않고**, 외부 모델이 만든 벡터를 **저장·인덱싱·검색**하는 역할을 담당한다.
- 전체 흐름은  
  **원본 데이터 → 임베딩 생성 → pgvector 저장 → 쿼리 임베딩 생성 → 유사도 검색 → LLM 응답**  
  구조로 설계하는 것이 가장 이상적이다.
- 클라우드에서는 OpenAI 계열, 온프레미스에서는 SentenceTransformers/KoSBERT/BGE/E5 계열 모델이 실무에서 많이 사용된다.
- 선택한 임베딩 모델의 차원 수에 맞춰 `vector(n)` 컬럼을 정의하고, 동일한 모델로 쿼리 임베딩도 생성해야 의미 있는 검색 결과를 얻을 수 있다.
- **임베딩이 필요한 이유:** 단어 매칭이 아닌 의미 기반 검색, 360배 빠른 속도, 비용 절감, 유사어/문맥 처리 가능

---